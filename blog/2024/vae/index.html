<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Variational Autoencoders: From Bayes' Theorem to Generative Modeling | Kelvin Christian </title> <meta name="author" content="Kelvin Christian"> <meta name="description" content="A rigorous mathematical journey through VAEs, starting from conditional probability and building up to practical implementations."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kelvinc123.github.io/blog/2024/vae/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kelvin</span> Christian </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/_pages/dropdown/">submenus </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Variational Autoencoders: From Bayes' Theorem to Generative Modeling</h1> <p class="post-meta"> Created in January 04, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/vae"> <i class="fa-solid fa-hashtag fa-sm"></i> VAE</a>   <a href="/blog/tag/generative-models"> <i class="fa-solid fa-hashtag fa-sm"></i> generative-models</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> probability</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction">Introduction</h2> <p>Variational Autoencoders (VAEs) <d-cite key="kingma2013auto"></d-cite> represent one of the most elegant intersections of probabilistic modeling and deep learning. Unlike traditional autoencoders that learn deterministic mappings, VAEs provide a principled framework for learning probabilistic latent variable models. This post will build VAEs from first principles, starting with conditional probability and carefully developing each component without mathematical gaps.</p> <h2 id="foundations-in-probability-theory">Foundations in Probability Theory</h2> <h3 id="conditional-probability-and-bayes-theorem">Conditional Probability and Bayes’ Theorem</h3> <p>Let’s start with the fundamental building blocks. Given two random variables $X$ and $Z$, the conditional probability of $X$ given $Z$ is:</p> \[P(X|Z) = \frac{P(X, Z)}{P(Z)}\] <p>From this, we can derive Bayes’ theorem:</p> \[P(Z|X) = \frac{P(X|Z)P(Z)}{P(X)}\] <p>where:</p> <ul> <li> <table> <tbody> <tr> <td>$P(Z</td> <td>X)$ is the <strong>posterior</strong> distribution</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$P(X</td> <td>Z)$ is the <strong>likelihood</strong> </td> </tr> </tbody> </table> </li> <li>$P(Z)$ is the <strong>prior</strong> distribution</li> <li>$P(X)$ is the <strong>marginal likelihood</strong> or <strong>evidence</strong> </li> </ul> <h3 id="the-marginal-likelihood">The Marginal Likelihood</h3> <p>The marginal likelihood is computed by marginalizing over all possible values of $Z$:</p> \[P(X) = \int P(X, Z) dZ = \int P(X|Z)P(Z) dZ\] <p>This integral is often intractable for complex models, which is the core challenge VAEs address.</p> <h2 id="the-problem-of-generative-modeling">The Problem of Generative Modeling</h2> <h3 id="setting-up-the-problem">Setting Up the Problem</h3> <p>Consider a dataset $\mathcal{D} = {x^{(1)}, x^{(2)}, …, x^{(N)}}$ of observations (e.g., images). We assume each observation $x^{(i)}$ is generated from some unobserved latent variable $z^{(i)}$ through the following process:</p> <ol> <li>Sample $z \sim p(z)$ from a prior distribution</li> <li> <table> <tbody> <tr> <td>Sample $x \sim p_\theta(x</td> <td>z)$ from a conditional distribution</td> </tr> </tbody> </table> </li> </ol> <p>Our goal is to:</p> <ol> <li> <table> <tbody> <tr> <td>Learn the parameters $\theta$ of the generative model $p_\theta(x</td> <td>z)$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Infer the posterior distribution $p_\theta(z</td> <td>x)$ for any given $x$</td> </tr> </tbody> </table> </li> </ol> <h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3> <p>The natural approach is to maximize the log-likelihood of the data:</p> \[\log p_\theta(\mathcal{D}) = \sum_{i=1}^N \log p_\theta(x^{(i)})\] <p>For a single data point:</p> \[\log p_\theta(x) = \log \int p_\theta(x|z)p(z) dz\] <h3 id="the-intractability-problem">The Intractability Problem</h3> <table> <tbody> <tr> <td>Computing $p_\theta(x)$ requires integrating over all possible values of $z$. For high-dimensional $z$ and complex $p_\theta(x</td> <td>z)$ (e.g., neural networks), this integral is intractable. Similarly, the posterior:</td> </tr> </tbody> </table> \[p_\theta(z|x) = \frac{p_\theta(x|z)p(z)}{p_\theta(x)}\] <p>is intractable due to the denominator.</p> <h2 id="variational-inference">Variational Inference</h2> <h3 id="the-variational-lower-bound">The Variational Lower Bound</h3> <table> <tbody> <tr> <td>To address intractability, we introduce an approximate posterior $q_\phi(z</td> <td>x)$ with parameters $\phi$. We’ll derive a lower bound on $\log p_\theta(x)$ using Jensen’s inequality.</td> </tr> </tbody> </table> <p>Starting from the log-likelihood:</p> \[\begin{align} \log p_\theta(x) &amp;= \log \int p_\theta(x, z) dz \\ &amp;= \log \int \frac{p_\theta(x, z)}{q_\phi(z|x)} q_\phi(z|x) dz \\ &amp;= \log \mathbb{E}_{q_\phi(z|x)}\left[\frac{p_\theta(x, z)}{q_\phi(z|x)}\right] \end{align}\] <p>Applying Jensen’s inequality (since $\log$ is concave):</p> \[\log p_\theta(x) \geq \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)}\right]\] <h3 id="the-evidence-lower-bound-elbo">The Evidence Lower Bound (ELBO)</h3> <p>Let’s expand this bound:</p> \[\begin{align} \mathcal{L}(\theta, \phi; x) &amp;= \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)}\right] \\ &amp;= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x, z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)] \\ &amp;= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \mathbb{E}_{q_\phi(z|x)}[\log p(z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)] \\ &amp;= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p(z)) \end{align}\] <p>where $D_{KL}$ is the Kullback-Leibler divergence:</p> \[D_{KL}(q \| p) = \mathbb{E}_q\left[\log \frac{q}{p}\right]\] <h3 id="tightness-of-the-bound">Tightness of the Bound</h3> <p>The gap between $\log p_\theta(x)$ and the ELBO is:</p> \[\log p_\theta(x) - \mathcal{L}(\theta, \phi; x) = D_{KL}(q_\phi(z|x) \| p_\theta(z|x))\] <p><strong>Proof:</strong></p> \[\begin{align} \log p_\theta(x) &amp;= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x)] \\ &amp;= \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{p_\theta(z|x)}\right] \\ &amp;= \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)} \cdot \frac{q_\phi(z|x)}{p_\theta(z|x)}\right] \\ &amp;= \mathcal{L}(\theta, \phi; x) + D_{KL}(q_\phi(z|x) \| p_\theta(z|x)) \end{align}\] <p>Since $D_{KL} \geq 0$, maximizing the ELBO provides a lower bound on the log-likelihood.</p> <h2 id="the-vae-framework">The VAE Framework</h2> <h3 id="architecture-components">Architecture Components</h3> <p>A VAE consists of two neural networks:</p> <ol> <li> <table> <tbody> <tr> <td> <strong>Encoder</strong> (recognition network): $q_\phi(z</td> <td>x)$</td> </tr> </tbody> </table> <ul> <li>Maps input $x$ to parameters of the approximate posterior</li> <li>Typically outputs mean $\mu_\phi(x)$ and variance $\sigma^2<em>\phi(x)$ for Gaussian $q</em>\phi$</li> </ul> </li> <li> <table> <tbody> <tr> <td> <strong>Decoder</strong> (generative network): $p_\theta(x</td> <td>z)$</td> </tr> </tbody> </table> <ul> <li>Maps latent code $z$ to parameters of the data distribution</li> <li>For continuous data: outputs mean (and optionally variance)</li> <li>For binary data: outputs Bernoulli parameters</li> </ul> </li> </ol> <h3 id="choice-of-distributions">Choice of Distributions</h3> <p><strong>Prior:</strong> Typically $p(z) = \mathcal{N}(0, I)$</p> <table> <tbody> <tr> <td> <strong>Approximate Posterior:</strong> $q_\phi(z</td> <td>x) = \mathcal{N}(\mu_\phi(x), \text{diag}(\sigma^2_\phi(x)))$</td> </tr> </tbody> </table> <p>For this choice, the KL divergence has a closed form:</p> \[D_{KL}(q_\phi(z|x) \| p(z)) = \frac{1}{2} \sum_{j=1}^J \left(1 + \log \sigma_j^2 - \mu_j^2 - \sigma_j^2\right)\] <p>where $J$ is the dimension of $z$.</p> <p><strong>Proof of KL divergence formula:</strong></p> <p>For two multivariate Gaussians $q = \mathcal{N}(\mu_1, \Sigma_1)$ and $p = \mathcal{N}(\mu_2, \Sigma_2)$:</p> \[D_{KL}(q \| p) = \frac{1}{2}\left[\log\frac{|\Sigma_2|}{|\Sigma_1|} - d + \text{tr}(\Sigma_2^{-1}\Sigma_1) + (\mu_2 - \mu_1)^T\Sigma_2^{-1}(\mu_2 - \mu_1)\right]\] <p>For our case with $p(z) = \mathcal{N}(0, I)$ and diagonal $q_\phi$, this simplifies to the formula above.</p> <h2 id="the-reparameterization-trick">The Reparameterization Trick</h2> <h3 id="the-gradient-problem">The Gradient Problem</h3> <p>To optimize the ELBO with respect to $\phi$, we need:</p> \[\nabla_\phi \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\] <p>We cannot simply move the gradient inside the expectation because the distribution depends on $\phi$.</p> <h3 id="the-solution">The Solution</h3> <table> <tbody> <tr> <td>Instead of sampling $z \sim q_\phi(z</td> <td>x)$, we reparameterize:</td> </tr> </tbody> </table> \[z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)\] <p>where $\odot$ denotes element-wise multiplication. Now:</p> \[\mathbb{E}_{q_\phi(z|x)}[f(z)] = \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)}[f(\mu_\phi(x) + \sigma_\phi(x) \odot \epsilon)]\] <p>The gradient can now be moved inside:</p> \[\nabla_\phi \mathbb{E}_{\epsilon}[f(\mu_\phi(x) + \sigma_\phi(x) \odot \epsilon)] = \mathbb{E}_{\epsilon}[\nabla_\phi f(\mu_\phi(x) + \sigma_\phi(x) \odot \epsilon)]\] <h2 id="implementation-in-pytorch">Implementation in PyTorch</h2> <p>Here’s a complete implementation of a VAE for MNIST:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">class</span> <span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">VAE</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># Encoder
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc21</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>  <span class="c1"># Mean
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc22</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>  <span class="c1"># Log variance
</span>        
        <span class="c1"># Decoder
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc21</span><span class="p">(</span><span class="n">h1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc22</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span>
    
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">h3</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc3</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc4</span><span class="p">(</span><span class="n">h3</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>

<span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Computes the VAE loss function.
    KL(q(z|x) || p(z)) = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    </span><span class="sh">"""</span>
    <span class="n">BCE</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">sum</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">KLD</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="p">.</span><span class="nf">exp</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">BCE</span> <span class="o">+</span> <span class="n">KLD</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">recon_batch</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">recon_batch</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Train Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s"> [</span><span class="si">{</span><span class="n">batch_idx</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span><span class="si">}</span><span class="s"> </span><span class="sh">'</span>
                  <span class="sa">f</span><span class="sh">'</span><span class="s">(</span><span class="si">{</span><span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_idx</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s">%)]</span><span class="se">\t</span><span class="s">Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">====&gt; Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s"> Average loss: </span><span class="si">{</span><span class="n">train_loss</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_samples</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Generate new samples from the learned distribution</span><span class="sh">"""</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="c1"># Sample from prior
</span>        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">20</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">).</span><span class="nf">cpu</span><span class="p">()</span>
        
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">view</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">visualize_latent_space</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Visualize the learned latent space</span><span class="sh">"""</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">latents</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">&gt;=</span> <span class="n">num_batches</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">mu</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
            <span class="n">latents</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">mu</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
            <span class="n">labels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">label</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    
    <span class="n">latents</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span><span class="n">latents</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># Plot first two dimensions
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">scatter</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">latents</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">latents</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">tab10</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">scatter</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Latent Dimension 1</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Latent Dimension 2</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Latent Space Visualization</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Main training script
</span><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="c1"># Hyperparameters
</span>    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">20</span>
    
    <span class="c1"># Setup
</span>    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Data loading
</span>    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">()])</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">MNIST</span><span class="p">(</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nc">MNIST</span><span class="p">(</span><span class="sh">'</span><span class="s">./data</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    
    <span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="c1"># Model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="nc">VAE</span><span class="p">(</span><span class="n">latent_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    
    <span class="c1"># Training
</span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
        
        <span class="c1"># Generate samples after each epoch
</span>        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Generating samples...</span><span class="sh">"</span><span class="p">)</span>
            <span class="nf">generate_samples</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># Visualize latent space
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Visualizing latent space...</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">visualize_latent_space</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
</code></pre></div></div> <h3 id="understanding-the-implementation">Understanding the Implementation</h3> <ol> <li> <strong>Encoder Network</strong>: Maps input to distribution parameters $(\mu, \log \sigma^2)$</li> <li> <strong>Reparameterization</strong>: Enables backpropagation through stochastic sampling</li> <li> <strong>Decoder Network</strong>: Maps latent codes back to data space</li> <li> <strong>Loss Function</strong>: Combines reconstruction loss and KL divergence</li> </ol> <p>The reconstruction term ensures the model can reconstruct inputs, while the KL term regularizes the latent space to match the prior.</p> <h2 id="applications-and-extensions">Applications and Extensions</h2> <h3 id="1-image-generation">1. Image Generation</h3> <p>VAEs learn smooth latent representations enabling:</p> <ul> <li> <strong>Interpolation</strong>: Smooth transitions between images</li> <li> <strong>Attribute manipulation</strong>: Modifying specific features</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Interpolate between two images in latent space</span><span class="sh">"""</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="c1"># Encode both images
</span>        <span class="n">mu1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">x1</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
        <span class="n">mu2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">x2</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
        
        <span class="c1"># Interpolate in latent space
</span>        <span class="n">interpolations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">):</span>
            <span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">mu1</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">mu2</span>
            <span class="n">recon</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">interpolations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">recon</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
        
        <span class="c1"># Visualize
</span>        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">recon</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">interpolations</span><span class="p">):</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">recon</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h3 id="2-semi-supervised-learning">2. Semi-Supervised Learning</h3> <p>By incorporating label information, VAEs can perform semi-supervised learning:</p> \[\mathcal{L} = \mathbb{E}_{q_\phi(z|x,y)}[\log p_\theta(x|z,y)] - D_{KL}(q_\phi(z|x,y) \| p(z|y))\] <h3 id="3-disentangled-representations">3. Disentangled Representations</h3> <p>$\beta$-VAE <d-cite key="higgins2017beta"></d-cite> encourages disentangled representations:</p> \[\mathcal{L}_{\beta} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta \cdot D_{KL}(q_\phi(z|x) \| p(z))\] <h3 id="4-conditional-vae-cvae">4. Conditional VAE (CVAE)</h3> <table> <tbody> <tr> <td>For conditional generation $p(x</td> <td>c)$:</td> </tr> </tbody> </table> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ConditionalVAE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">condition_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">latent_dim</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ConditionalVAE</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># Encoder
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_dim</span> <span class="o">+</span> <span class="n">condition_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc21</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc22</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        
        <span class="c1"># Decoder
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">latent_dim</span> <span class="o">+</span> <span class="n">condition_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc21</span><span class="p">(</span><span class="n">h1</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc22</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">z</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">h3</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc3</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc4</span><span class="p">(</span><span class="n">h3</span><span class="p">))</span>
</code></pre></div></div> <h3 id="5-applications-in-other-domains">5. Applications in Other Domains</h3> <ul> <li> <strong>Drug Discovery</strong>: Generating molecular structures</li> <li> <strong>Text Generation</strong>: Learning sentence embeddings</li> <li> <strong>Music Generation</strong>: Creating musical sequences</li> <li> <strong>Anomaly Detection</strong>: Using reconstruction error</li> </ul> <h2 id="advanced-topics">Advanced Topics</h2> <h3 id="importance-weighted-autoencoders-iwae">Importance Weighted Autoencoders (IWAE)</h3> <p>IWAE <d-cite key="burda2015importance"></d-cite> uses multiple samples to tighten the bound:</p> \[\mathcal{L}_k = \mathbb{E}_{z_1,...,z_k \sim q_\phi(z|x)}\left[\log \frac{1}{k} \sum_{i=1}^k \frac{p_\theta(x, z_i)}{q_\phi(z_i|x)}\right]\] <h3 id="normalizing-flows">Normalizing Flows</h3> <table> <tbody> <tr> <td>Enhance the expressiveness of $q_\phi(z</td> <td>x)$ using invertible transformations:</td> </tr> </tbody> </table> \[z_K = f_K \circ ... \circ f_1(z_0), \quad z_0 \sim q_0(z_0|x)\] <h3 id="hierarchical-vaes">Hierarchical VAEs</h3> <p>Use multiple layers of latent variables:</p> \[p(x, z_1, z_2) = p(x|z_1)p(z_1|z_2)p(z_2)\] <h2 id="practical-considerations">Practical Considerations</h2> <h3 id="1-posterior-collapse">1. Posterior Collapse</h3> <p>Sometimes the model ignores the latent code ($q_\phi(z|x) \approx p(z)$). Solutions:</p> <ul> <li>KL annealing: Gradually increase KL weight during training</li> <li>Free bits: Ensure minimum information in latent code</li> <li>Architectural changes: Skip connections, powerful decoders</li> </ul> <h3 id="2-reconstruction-quality">2. Reconstruction Quality</h3> <p>VAEs often produce blurry reconstructions due to:</p> <ul> <li>Gaussian likelihood assumption</li> <li>Averaging effect of sampling</li> <li>Trade-off with KL regularization</li> </ul> <h3 id="3-hyperparameter-tuning">3. Hyperparameter Tuning</h3> <p>Key hyperparameters:</p> <ul> <li>Latent dimension: Balance expressiveness and regularization</li> <li>Network architecture: Deeper networks for complex data</li> <li>Learning rate: Often requires careful scheduling</li> <li>$\beta$ weight: For $\beta$-VAE variants</li> </ul> <h2 id="theoretical-connections">Theoretical Connections</h2> <h3 id="information-theory-perspective">Information Theory Perspective</h3> <p>The ELBO can be rewritten as:</p> \[\mathcal{L} = \mathbb{E}_{p_{\text{data}}(x)}[\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]] - I_q(X; Z)\] <p>where $I_q(X; Z)$ is the mutual information under $q_\phi$.</p> <h3 id="connection-to-rate-distortion-theory">Connection to Rate-Distortion Theory</h3> <p>VAEs optimize a trade-off similar to rate-distortion:</p> <ul> <li> <table> <tbody> <tr> <td> <strong>Rate</strong>: $D_{KL}(q_\phi(z</td> <td>x) | p(z))$ (bits to encode $z$)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td> <strong>Distortion</strong>: $-\mathbb{E}<em>{q</em>\phi(z</td> <td>x)}[\log p_\theta(x</td> <td>z)]$ (reconstruction error)</td> </tr> </tbody> </table> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Variational Autoencoders provide a principled framework for learning generative models by combining:</p> <ol> <li> <strong>Variational inference</strong> for approximate posterior computation</li> <li> <strong>Neural networks</strong> for flexible function approximation</li> <li> <strong>Reparameterization trick</strong> for efficient gradient estimation</li> </ol> <p>The mathematical foundation we’ve developed—from conditional probability through the ELBO derivation—reveals VAEs as a natural solution to the challenging problem of learning latent variable models. Their flexibility and theoretical grounding have made them a cornerstone of modern generative modeling, with applications ranging from image synthesis to drug discovery.</p> <p>The beauty of VAEs lies not just in their practical success, but in how they unite probabilistic modeling with deep learning, providing both theoretical guarantees and empirical performance. As we continue to develop more sophisticated variants and applications, the core insights remain: by approximating intractable posteriors and optimizing tractable bounds, we can learn rich generative models of complex data.</p> <h2 id="references">References</h2> <d-bibliography></d-bibliography> <h2 id="appendix-mathematical-proofs">Appendix: Mathematical Proofs</h2> <h3 id="proof-of-elbo-as-lower-bound">Proof of ELBO as Lower Bound</h3> <table> <tbody> <tr> <td> <strong>Theorem:</strong> For any distributions $p_\theta(x, z)$ and $q_\phi(z</td> <td>x)$:</td> <td> </td> </tr> <tr> <td>$$\log p_\theta(x) \geq \mathbb{E}<em>{q</em>\phi(z</td> <td>x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z</td> <td>x)}\right]$$</td> </tr> </tbody> </table> <p><strong>Proof:</strong> \(\begin{align} \log p_\theta(x) &amp;= \log \int p_\theta(x, z) dz \\ &amp;= \log \int \frac{p_\theta(x, z)}{q_\phi(z|x)} q_\phi(z|x) dz \\ &amp;\geq \int q_\phi(z|x) \log \frac{p_\theta(x, z)}{q_\phi(z|x)} dz \quad \text{(Jensen's inequality)} \\ &amp;= \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)}\right] \end{align}\)</p> <h3 id="gaussian-kl-divergence-derivation">Gaussian KL Divergence Derivation</h3> <p>For $q = \mathcal{N}(\mu_1, \sigma_1^2 I)$ and $p = \mathcal{N}(\mu_2, \sigma_2^2 I)$:</p> \[\begin{align} D_{KL}(q \| p) &amp;= \int q(z) \log \frac{q(z)}{p(z)} dz \\ &amp;= \mathbb{E}_q[\log q(z)] - \mathbb{E}_q[\log p(z)] \\ &amp;= -\frac{d}{2}\log(2\pi) - \frac{1}{2}\sum_i \log \sigma_{1,i}^2 - \frac{d}{2} \\ &amp;\quad + \frac{d}{2}\log(2\pi) + \frac{1}{2}\sum_i \log \sigma_{2,i}^2 + \frac{1}{2\sigma_{2}^2}\mathbb{E}_q[\|z - \mu_2\|^2] \\ &amp;= \frac{1}{2}\sum_i \left[\log \frac{\sigma_{2,i}^2}{\sigma_{1,i}^2} - 1 + \frac{\sigma_{1,i}^2}{\sigma_{2,i}^2} + \frac{(\mu_{1,i} - \mu_{2,i})^2}{\sigma_{2,i}^2}\right] \end{align}\] <p>For the special case where $p = \mathcal{N}(0, I)$, this simplifies to our formula.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/generative-model/">Exploring the Landscape of Generative Models: A Simple Guide</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/kl-mle/">The Dual Nature of MLE and KL Divergence in Generative Modeling</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"kelvinc123/kelvinc123.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Kelvin Christian. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"It is a resume :)",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-submenus",title:"submenus",description:"",section:"Navigation",handler:()=>{window.location.href="/_pages/dropdown/"}},{id:"post-variational-autoencoders-from-bayes-39-theorem-to-generative-modeling",title:"Variational Autoencoders: From Bayes' Theorem to Generative Modeling",description:"A rigorous mathematical journey through VAEs, starting from conditional probability and building up to practical implementations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/vae/"}},{id:"post-exploring-the-landscape-of-generative-models-a-simple-guide",title:"Exploring the Landscape of Generative Models: A Simple Guide",description:"Simple comparison between Autoregressive, Flow, VAEs, GANs, Energy-Based, Score-Based, and Diffusion models.",section:"Posts",handler:()=>{window.location.href="/blog/2023/generative-model/"}},{id:"post-the-dual-nature-of-mle-and-kl-divergence-in-generative-modeling",title:"The Dual Nature of MLE and KL Divergence in Generative Modeling",description:"Explaining the connection between MLE and KL Divergence in Generative Modeling",section:"Posts",handler:()=>{window.location.href="/blog/2023/kl-mle/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-10",title:"project 10",description:"A project with an introduction section",section:"Projects",handler:()=>{window.location.href="/projects/10_project/"}},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"projects-askcet",title:"ASKCET",description:"A product recognition app.",section:"Projects",handler:()=>{window.location.href="/projects/askcet/"}},{id:"projects-demat",title:"DeMAT",description:"Diffusion-enhanced Mask Aware Transformer for Deep Generative In-painting",section:"Projects",handler:()=>{window.location.href="/projects/demat/"}},{id:"projects-fine-tuning-stable-diffusion",title:"Fine-Tuning Stable Diffusion",description:"Application of LoRA and Dreambooth for fine-tuning Stable Diffusion",section:"Projects",handler:()=>{window.location.href="/projects/ft-stable-diffusion/"}},{id:"projects-isearch-hotels",title:"iSEArch Hotels",description:"Chatbot for hotel recommendations",section:"Projects",handler:()=>{window.location.href="/projects/hotel-chatbot/"}},{id:"projects-multi-subtask-policy-learning-in-robotic",title:"Multi-subtask Policy Learning in robotic",description:"Data-Efficient Multi-subtask Policy Learning for Robotic Manipulation",section:"Projects",handler:()=>{window.location.href="/projects/policy-learning/"}},{id:"projects-soba-up",title:"Soba-Up",description:"Be safe on a night out!",section:"Projects",handler:()=>{window.location.href="/projects/soba-up/"}},{id:"projects-tennis-shot-classification",title:"Tennis Shot Classification",description:"Trained TimeSformer model using a tennis video clip dataset",section:"Projects",handler:()=>{window.location.href="/projects/tennis-shot/"}},{id:"projects-text-to-table",title:"Text-to-Table",description:"Text-to-Table: Comparison between GPT and BART",section:"Projects",handler:()=>{window.location.href="/projects/text-to-table/"}},{id:"projects-trip-optimizer",title:"Trip Optimizer",description:"An automated trip planner given the restaurants and recreational spots data.",section:"Projects",handler:()=>{window.location.href="/projects/trip/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6B%65%6C%76%69%6E%63%68%72%69%73%74%69%61%6E%32%37%37@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>