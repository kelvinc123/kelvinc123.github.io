<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Exploring the Landscape of Generative Models: A Simple Guide | Kelvin Christian </title> <meta name="author" content="Kelvin Christian"> <meta name="description" content="Simple comparison between Autoregressive, Flow, VAEs, GANs, Energy-Based, Score-Based, and Diffusion models."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kelvinc123.github.io/blog/2023/generative-model/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kelvin</span> Christian </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/_pages/dropdown/">submenus </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Exploring the Landscape of Generative Models: A Simple Guide</h1> <p class="post-meta"> Created in November 20, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/vae"> <i class="fa-solid fa-hashtag fa-sm"></i> VAE</a>   <a href="/blog/tag/autoregressive"> <i class="fa-solid fa-hashtag fa-sm"></i> Autoregressive</a>   <a href="/blog/tag/flow"> <i class="fa-solid fa-hashtag fa-sm"></i> Flow</a>   <a href="/blog/tag/energy-based"> <i class="fa-solid fa-hashtag fa-sm"></i> Energy-Based</a>   <a href="/blog/tag/score-based"> <i class="fa-solid fa-hashtag fa-sm"></i> Score-Based</a>   <a href="/blog/tag/gan"> <i class="fa-solid fa-hashtag fa-sm"></i> GAN</a>   <a href="/blog/tag/diffusion"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion</a>   ·   <a href="/blog/category/generative-models"> <i class="fa-solid fa-tag fa-sm"></i> generative-models</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>\(\DeclareMathOperator*{\argmin}{argmin}\) \(\DeclareMathOperator*{\argmax}{argmax}\)</p> <p>This blog offers a concise comparison of the most widely used generative models: Autoregressive Models, Normalizing Flow Models, Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Energy-Based Models (EBMs), Score-Based Models, and the recently emerging Diffusion Models. Each of these models shared a common goal: to effectively model and represent the probability distribution \(p_{\theta}(\mathbf{x})\) of complex, high-dimensional dataset.</p> <p><br></p> <h2 id="autoregressive">Autoregressive</h2> <p>Autoregressive (AR) models use the chain rule to model the probability of a high-dimensional dataset. For a given high-dimensional dataset \(\mathbf{x} \in \mathbb{R}^{n}\), the probability can be broken down into the product of conditional probabilities of <strong>each dimension given the preceding elements</strong>. More formally,</p> \[p_{\theta}(\mathbf{x}) = \prod_{i=1}^{n}p_{\theta}(x_{i} | x_{1}, x_{2}, \dots, x_{i-1})\] <p>This probabilistic breakdown facilitates the application of various assumptions, like the Markov assumption, or the integration of neural network architectures such as LSTMs or Transformer models. This decomposition allows the model to learn the probability distribution quickly and accurately. Autoregressive models have proven effective in generating sequential data, such as language modeling and time-series forecasting.</p> <h4 id="pros">Pros</h4> <ul> <li> <b>Likelihood Estimation </b>: Provides the exact likelihood calculation as a result of chain rule</li> </ul> <h4 id="cons">Cons</h4> <ul> <li> <b>Sampling speed</b>: Slow sampling speed due to the nature of sequential generation; each step requires the output from previous steps</li> <li> <b>Scalability</b>: Struggles in handling a very large dataset and long sequences</li> </ul> <h4 id="references">References</h4> <ul> <li> <strong>MADE: Masked Autoencoder for Distribution Estimation</strong> by Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. <a href="https://arxiv.org/abs/1502.03509" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Attention Is All You Need</strong> by Ashish Vaswani et al. <a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Pixel Recurrent Neural Networks</strong> by Aaron van den Oord and others. <a href="https://arxiv.org/abs/1601.06759" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Conditional Image Generation with PixelCNN Decoders</strong> by Aaron van den Oord and others. <a href="https://arxiv.org/abs/1606.05328" rel="external nofollow noopener" target="_blank">Link</a>.</li> </ul> <p><br></p> <h2 id="normalizing-flow-models">Normalizing Flow Models</h2> <p>Normalizing Flow Models use a simple latent variable \(\mathbf{z}\), typically Gaussian, alongside a deterministic, invertible function \(f_{\theta}\) to model the probability distribution of the dataset \(p_{\theta}(\mathbf{x})\). The function \(f_{\theta}\) is defined so that \(f_{\theta}(\mathbf{z}) = \mathbf{x}\) and \(f_{\theta}^{-1}(\mathbf{x}) = \mathbf{z}\). The latent variable \(\mathbf{z}\) must match the dimensionality of \(\mathbf{x}\) to ensure that \(f_{\theta}\) is invertible. Note that the function \(f\) can be a composition of multiple invertible functions, \(f = f_{1} \circ f_{2} \dots \circ f_{n}\), enabling the construction of a complex function from simpler ones. The density of \(\mathbf{x}\) can then be formulated using the change of variable method:</p> \[p_{X}(\mathbf{x} ; \theta) = p_{Z}(\mathbf{z}) | \text{det} J_{f_{\theta}^{-1}}(\mathbf{z})|\] <p><br> Here, \(\mathbf{z} = f_{\theta}^{-1}(\mathbf{x})\) and \(|\text{det} J_{f^{-1}}(\mathbf{z})|\) is the absolute value of the determinant of the jacobian matrix \(f^{-1}\).</p> <p>To evaluate the density of \(\mathbf{x}\), the mapping function \(f\) is used to transform \(\mathbf{x}\) to \(\mathbf{z}\) and use the right-hand side equation above to calculate the density \(p_{X}(\mathbf{x}; \theta)\). The simplicity of \(p_{Z}(\mathbf{z})\) is the key to computing \(p_{X}(\mathbf{x}; \theta)\). To generate new sample from \(p_{X}(\mathbf{x}; \theta)\), we can sample \(\mathbf{z}\) from the latent distribution \(p_{Z}(\mathbf{z})\) and apply \(f\) to obtain \(f(\mathbf{z}) = \mathbf{x}\).</p> <h4 id="pros-1">Pros</h4> <ul> <li> <b>Likelihood Estimation </b>: Provides the exact likelihood calculation due to the change of variable formula</li> </ul> <h4 id="cons-1">Cons</h4> <ul> <li> <b>Limitation in function selection</b>: The requirement for the mapping function f to be invertible and to maintain the same input-output dimension restrict the choice of functions</li> </ul> <h4 id="references-1">References</h4> <ul> <li> <strong>Masked Autoregressive Flow for Density Estimation</strong> by George Papamakarios, Theo Pavlakou, and Iain Murray. <a href="https://arxiv.org/abs/1705.07057" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Normalizing Flows for Probabilistic Modeling and Inference</strong> by George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. <a href="https://arxiv.org/abs/1912.02762" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>NICE: Non-linear Independent Components Estimation</strong> by Laurent Dinh, David Krueger, and Yoshua Bengio. <a href="https://arxiv.org/abs/1410.8516" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Gaussianization Flows</strong> by Chenlin Meng, Yang Song, Jiaming Song, and Stefano Ermon. <a href="https://arxiv.org/abs/2003.01941" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Density Estimation Using Real NVP</strong> by Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. <a href="https://arxiv.org/abs/1605.08803" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Glow: Generative Flow with Invertible 1x1 Convolutions</strong> by Diederik P. Kingma, and Prafulla Dhariwal. <a href="https://arxiv.org/abs/1807.03039" rel="external nofollow noopener" target="_blank">Link</a>.</li> </ul> <p><br></p> <h2 id="variational-autoencoders">Variational Autoencoders</h2> <p>Variational Autoencoders (VAEs) are a class of generative model that models the probability distribution \(p_{\theta}(\mathbf{x})\) by learning a latent representation \(\mathbf{z}\) of the input data \(\mathbf{x}\). They consist of two main components: an encoder that maps input data to a latent distribution and a decoder that reconstructs the data from this latent space. Unlike the Normalizing Flow Model, this latent variable \(\mathbf{z}\) typically has a smaller dimension than the input data \(\mathbf{x}\). VAEs learn to maximize the evidence lower bound (ELBO) of the log probability of the dataset using the following formula:</p> \[\begin{align*} \log p_{\theta}(\mathbf{x}) &amp;\ge \textbf{ELBO}(\mathbf{x}, \theta, \phi) \\ &amp;= \mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}\left[\log\frac{p_{\theta}(\mathbf{x}, \mathbf{z})}{q_{\phi}(\mathbf{z} | \mathbf{x})}\right] \\ &amp;= \mathbb{E}_{q_{\phi}(\mathbf{z} | \mathbf{x})}[\log p_{\theta}(\mathbf{x}| \mathbf{z})] - D_{\text{KL}}(q_{\phi}(\mathbf{z} | \mathbf{x}) || p(\mathbf{z})) \end{align*}\] <p><br> In this setup, the \(p(\mathbf{z})\) is usually a simple distribution like Gaussian. The term \(q_{\phi}(\mathbf{z} | \mathbf{x})\) and \(p_{\theta}(\mathbf{x} | \mathbf{z})\) are usually called the encoder and decoder respectively. To sample from VAEs, we first need to sample the latent \(\mathbf{z}\) from \(p(\mathbf{z})\) and use the decoder component \(p_{\theta}(\mathbf{x} | \mathbf{z})\) to transform \(z\) to the sample.</p> <h4 id="pros-2">Pros</h4> <ul> <li> <b>Latent Representation</b>: Creates a meaningful lower-dimensional representation of the input data</li> <li> <b>Sampling speed</b>: Fast sampling speed, requiring only a forward pass through the decoder</li> </ul> <h4 id="cons-2">Cons</h4> <ul> <li> <b>Approximate of true distribution</b>: Doesn't exactly compute the true distribution; it uses the approximation via ELBO</li> </ul> <h4 id="references-2">References</h4> <ul> <li> <strong>Auto-Encoding Variational Bayes</strong> by Diederik P Kingma and Max Welling. <a href="https://arxiv.org/abs/1312.6114" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Disentangling Disentanglement in Variational Autoencoders</strong> by Emile Mathieu, Tom Rainforth, N. Siddharth, Yee Whye Teh. <a href="https://arxiv.org/abs/1812.02833" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Understanding disentangling in beta-VAE</strong> by Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, Alexander Lerchner. <a href="https://arxiv.org/abs/1804.03599" rel="external nofollow noopener" target="_blank">Link</a>.</li> </ul> <p><br></p> <h2 id="generative-adversarial-networks">Generative Adversarial Networks</h2> <p>Generative Adversarial Networks (GANs) are generative models that do not directly model the probability distribution \(p_{\theta}(\mathbf{x})\) of the dataset. Instead, GANs focus on producing high-quality samples. GANs have two main components: a discriminator \(D\) and a generator \(G\). The discriminator acts as a binary classifier distinguishing real from generated samples. At the same time, the generator tries to fool the discriminator by generating fake samples from a random variable \(\mathbf{z}\) drawn from a prior distribution \(p(\mathbf{z})\). These two components can be formulated as a minimax game with the value function:</p> \[\min_{G} \max_{D}V(D, G) = \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log D(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})}[\log (1 - D(G(\mathbf{z})))]\] <p><br> In this adversarial setup, \(G\) and \(D\) are updated sequentially. The discriminator sharpens its ability to distinguish real from fake, while the generator improves at creating increasingly convincing fake data. To generate new samples, we first sample \(\mathbf{z} \sim p(\mathbf{z})\) and then feed it into the generator.</p> <h4 id="pros-3">Pros</h4> <ul> <li> <b>Sample quality</b>: Known for producing high-quality samples as a result of the minimax game</li> <li> <b>Sampling speed</b>: Fast sampling speed, requiring only a forward pass through the generator</li> </ul> <h4 id="cons-3">Cons</h4> <ul> <li> <b>Lack of probability estimation</b>: GANs do not focus maximum likelihood estimation, hence are unable to compute probabilities or bounds like VAEs</li> <li> <b>Mode collapse</b>: Sometimes the generator only produces the limited variety of outputs</li> <li> <b>Training complexity</b>: GANs are difficult to train, sensitive to the hyperparameters, and often face stability issues</li> </ul> <h4 id="references-3">References</h4> <ul> <li> <strong>Generative Adversarial Networks</strong> by Ian J. Goodfellow and colleagues. <a href="https://arxiv.org/abs/1406.2661" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Wasserstein GAN</strong> introducing a new training approach for GANs. <a href="https://arxiv.org/abs/1701.07875" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</strong> by Sebastian Nowozin, Botond Cseke, Ryota Tomioka. <a href="https://arxiv.org/abs/1606.00709" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Conditional Generative Adversarial Nets</strong> by Mehdi Mirza, Simon Osindero. <a href="https://arxiv.org/abs/1411.1784" rel="external nofollow noopener" target="_blank">Link</a>.</li> </ul> <p><br></p> <h2 id="energy-based-models">Energy-Based Models</h2> <p>Energy-Based Models (EBMs) estimate the probability distribution of a dataset \(p_{\theta}(\mathbf{x})\) directly using the energy function \(E_{\theta}(\mathbf{x})\). Unlike the other generative models, the energy function defines an unnormalized negative log probability, which is less restrictive. To ensure that \(p_{\theta}(\mathbf{x})\) is a valid probability function, it must be nonnegative and integrate to \(1\). Therefore, the probability density function of EBM is expressed as:</p> \[p_{\theta}(\mathbf{x}) = \frac{\exp(-E_{\theta}(\mathbf{x}))}{Z_{\theta}}\] <p>Where \(Z_{\theta}\) is the normalizing constant (or partition function) for the exponentiation of the energy function. Although the \(Z_{\theta}\) here is constant, it’s still a function of \(\theta\). For training EBMs, we can use maximum likelihood learning. The gradient of the \(\log p_{\theta}(\mathbf{x})\) w.r.t \(\theta\) is:</p> \[\begin{align*} \nabla_{\theta}\log p_{\theta}(\mathbf{x}) &amp;= -\nabla_{\theta}E_{\theta}(\mathbf{x}) - \mathbb{E}_{\mathbf{x}_{\text{sample}} \sim p_{\theta}(\mathbf{x})}[\nabla_{\theta}E_{\theta}(\mathbf{x}_{\text{sample}})] \\ &amp;\approx -\nabla_{\theta}E_{\theta}(\mathbf{x}) - \nabla_{\theta}E_{\theta}(\mathbf{x}_{\text{sample}}) \end{align*}\] <p>Here, \(\mathbf{x}_{\text{sample}}\) is drawn from our EBM \(p_{\theta}(\mathbf{x_{\text{sample}}})\). Unfortunately, the sampling process can be challenging as it needs the MCMC methods such as Metropolish Hastings or Langevin dynamics. The iterative Langevin MCMC sampling procedure is:</p> \[\mathbf{x}^{(k+1)} \leftarrow \mathbf{x}^{(k)} + \frac{\epsilon}{2} \nabla_{\mathbf{x}}\log p_{\theta}(\mathbf{x}^{(k)}) + \epsilon \mathbf{z}^{(k)}\] <p>Where \(\mathbf{z}^{k}\) is a standard Gaussian random variable. This process converges to the EBM’s distribution \(p_{\theta}(\mathbf{x})\) as \(\epsilon \rightarrow 0\) and \(k \rightarrow \infty\).</p> <h4 id="pros-4">Pros</h4> <ul> <li> <b>Energy function architecture</b>: Flexibility in choosing the energy function</li> <li> <b>Direct modeling of probability</b>: Capable of modeling the probability distribution directly, without the need for an intermediate latent space</li> <li> <b>Robust to overfitting</b>: EBMs are generally more robust to overfitting compared to other generative models</li> </ul> <h4 id="cons-4">Cons</h4> <ul> <li> <b>Sampling speed</b>: The need for the MCMC sampling method to converge makes sampling very slow</li> <li> <b>Training speed</b>: Each training iteration requires sampling, which is very slow</li> <li> <b>Difficulty in probability estimation</b>: While EBMs model probabilities directly, calculating the partition function is often intractable</li> </ul> <h4 id="references-4">References</h4> <ul> <li> <strong>How to Train Your Energy-Based Models</strong> by Yang Song, Diederik P. Kingma. <a href="https://arxiv.org/abs/2101.03288" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Flow Contrastive Estimation of Energy-Based Models</strong> by Ruiqi Gao, Erik Nijkamp, Diederik P. Kingma, Zhen Xu, Andrew M. Dai, Ying Nian Wu. <a href="https://arxiv.org/abs/1912.00589" rel="external nofollow noopener" target="_blank">Link</a>.</li> </ul> <p><br></p> <h2 id="score-based-models">Score-Based Models</h2> <p>Score-Based models offer a unique approach to generative modeling. Unlike the other traditional generative models, Score-Based Models do not explicitly model the probability distribution of the dataset. Instead, they focus on approximating the gradient of the log probability \(\nabla_{\mathbf{x}}\log p_{\theta}(\mathbf{x})\), through a score function \(s_{\theta}(\mathbf{x})\). One method to train these models is by learning the <em>Noise Conditional Score Networks</em> (NCSN), denoted by \(s_{\theta}(\mathbf{x}, \sigma)\), where the goal is to predict scores for perturbed datasets at varying noise levels, \(\sigma\). The objective function for a given noise level \(\sigma\) is:</p> \[\mathcal{l}(\theta; \sigma) = \mathbb{E}_{p_{\text{data}}}\mathbb{E}_{\tilde{\mathbf{x}}\sim\mathcal{N}(\mathbf{x}, \sigma^{2}I)}\left[\left|\left|s_{\theta}(\tilde{\mathbf{x}}, \sigma) + \frac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^{2}}\right|\right|^{2}_{2}\right]\] <p>This function is based on the denoising score matching technique, where training the score function of the data with added noise is equivalent to training the denoising objective. The denoising objective’s goal is to gradually denoise data, starting from a pure noise distribution. The combined objective function across different levels of \(\sigma\) is:</p> \[\mathcal{L}(\theta; \{\sigma_{i}\}_{i=1}^{L}) = \frac{1}{L}\sum_{i=1}^{L}\lambda(\sigma_{i})\mathcal{l}(\theta; \sigma_{i})\] <p>To generate samples from the NCSN, the <em>Annealed Langevin Dynamics</em> is employed. The idea is to run Langevin dynamics iterations scaled by the noise level for each noise level. The <em>Annealed Langevin Dynamics</em> algorithm can be seen below:</p> <figure text-align="center" margin="0 auto" display="block"> <img class="img-fluid rounded z-depth-1" src="/assets/img/annealed-ld.jpg" alt="Annealed Langevin Dynamics Algorithm" width="350" data-zoomable=""> <figcaption>Langevin Dynamics Algorithm. (Image source: <a href="https://arxiv.org/abs/1907.05600" target="_blank" data-zoomable="" rel="external nofollow noopener">Song, Y., &amp; Ermon, S. (2019)</a>)</figcaption> </figure> <h4 id="pros-5">Pros</h4> <ul> <li> <b>Sample quality</b>: Produces superior results compared to the other generative models</li> <li> <b>Flexible model architecture</b>: The score function's architecture is less constrained, allowing for various designs as long as input and output dimensions match</li> <li> <b>Training simplicity</b>: Leveraging denoising tasks simplifies the training process</li> </ul> <h4 id="cons-5">Cons</h4> <ul> <li> <b>Sampling speed</b>: The reliance on Annealed Langevin Dynamics for sampling can be time-consuming</li> <li> <b>Lack in probability estimation</b>: The indirect modeling approach complicates exact probability distribution calculations</li> </ul> <h4 id="references-5">References</h4> <ul> <li> <strong>Generative Modeling by Estimating Gradients of the Data Distribution</strong> by Yang Song and Stefano Ermon. <a href="https://arxiv.org/abs/1907.05600" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Improved Techniques for Training Score-Based Generative Models</strong> by Yang Song and Stefano Ermon. <a href="https://arxiv.org/abs/2006.09011" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Score-Based Generative Modeling through Stochastic Differential Equations</strong> by Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. <a href="https://arxiv.org/abs/2011.13456" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Maximum Likelihood Training of Score-Based Diffusion Models</strong> by Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. <a href="https://arxiv.org/abs/2101.09258" rel="external nofollow noopener" target="_blank">Link</a>.</li> </ul> <p><br></p> <h2 id="diffusion-models">Diffusion Models</h2> <p>Diffusion Models stand as a pinnacle in the field of generative models. These models operate through a unique process that gradually adds Gaussian noise to the dataset until the data is purely random. This process of adding noises is called the <em>forward process</em>. More formally, for a given initial dataset \(\mathbf{x}_{0}\), Diffusion Models sequentially add a Gaussian noise, adhering to the Markov assumption. This is mathematically represented as:</p> \[q(x_{0}) = p_{\text{data}}(\mathbf{x}_{0})\] <p>and</p> \[q(\mathbf{x}_{1:T} | \mathbf{x}_{0}) \prod_{i=1}^{T}q(\mathbf{x}_{t} | \mathbf{x}_{t-1}), \hspace{1.5cm} q(\mathbf{x}_{t} | \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1 - \beta_{t}}\mathbf{x}_{t-1}, \beta_{t}I)\] <p>where \(\beta_{t}\) is a noise level parameters.</p> <p>Diffusion Models mirror the principle of Variational Autoencoders (VAEs) in learning the probability distribution \(p_{\theta}(\mathbf{x}_{0})\). This is achieved by maximizing the Evidence Lower Bound (ELBO), expressed as:</p> \[\begin{align*} \log p_{\theta}(\mathbf{x}_{0}) &amp;\ge \textbf{ELBO}(\mathbf{x}, \theta) \\ &amp;= \mathbb{E}_{q}\left[\log\frac{p_{\theta}(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} | \mathbf{x}_{0})}\right] \\ \end{align*}\] <p>Here, the joint probability \(p_{\theta}(\mathbf{x}_{0:T})\), also defined under the Markov assumption as follows:</p> \[p_{\theta}(\mathbf{x}_{0:T}) = p(\mathbf{x}_{T}) \prod_{i=1}^{T}p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t}), \hspace{1.5cm} p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_{t}) = \mathcal{N}(\mu_{\theta}(\mathbf{x}_{t}, t), \Sigma_{\theta}(\mathbf{x}_{t}, t))\] <p>This joint distribution signifies the <em>reverse process</em>, which attempts to revert the forward process by denoising the noise.</p> <p>Note that the difference between VAEs and Diffusion Models lies in the distribution of \(q\). In the Diffusion Model, \(q\) is fixed and doesn’t depend on any trainable parameters. This framework aligns with the training objective of Score-Based Models, essentially focusing on learning the denoising process. The training objective can be simplifies to learning the noise \(\epsilon(\mathbf{x}_{t}, t)\) at a specific time \(t\) by minimizing the following simplified loss:</p> \[\mathcal{L}(\theta) := \mathbb{E}_{t, \mathbf{x}_{0}, \epsilon}\left[\left|\left|\epsilon - \epsilon(\mathbf{x}_{t}, t)\right|\right|_{2}^{2}\right]\] <p>To sample from the Diffusion Models, one begins by sampling random noise from the distribution \(p(\mathbf{x}_{T})\) and gradually takes a denoising step until \(\mathbf{x}_{0}\) is formed. This sampling procedure is similar to the denoising score matching process. Below is the sampling algorithm for Diffusion Models:</p> <figure text-align="center" margin="0 auto" display="block"> <img class="img-fluid rounded z-depth-1" src="/assets/img/ddpm-sampling.jpg" alt="Diffusion Models Sampling Algorithm" width="350" data-zoomable=""> <figcaption>Diffusion Model Sampling Algorithm. (Image source: <a href="https://arxiv.org/abs/2006.11239" target="_blank" data-zoomable="" rel="external nofollow noopener">Jonathan Ho, Ajay Jain, and Pieter Abbeel (2020)</a>)</figcaption> </figure> <h4 id="pros-6">Pros</h4> <ul> <li> <b>Sample quality</b>: Excel in producing high-quality and detailed samples</li> <li> <b>Latent Representation</b>: Able to get a latent representation of the dataset</li> </ul> <h4 id="cons-6">Cons</h4> <ul> <li> <b>Sample speed</b>: The denoising steps makes sampling very slow</li> </ul> <h4 id="references-6">References</h4> <ul> <li> <strong>Denoising Diffusion Probabilistic Models</strong> by Jonathan Ho, Ajay Jain, Pieter Abbeel. <a href="https://arxiv.org/abs/2006.11239" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Improved Denoising Diffusion Probabilistic Models</strong> by Alex Nichol, Prafulla Dhariwal. <a href="https://arxiv.org/abs/2102.09672" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Diffusion Models Beat GANs on Image Synthesis</strong> by Prafulla Dhariwal, Alex Nichol. <a href="https://arxiv.org/abs/2105.05233" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Denoising Diffusion Implicit Models</strong> by Jiaming Song, Chenlin Meng, Stefano Ermon. <a href="https://arxiv.org/abs/2010.02502" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>High-Resolution Image Synthesis with Latent Diffusion Models</strong> by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. <a href="https://arxiv.org/abs/2112.10752" rel="external nofollow noopener" target="_blank">Link</a>.</li> <li> <strong>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</strong> by Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi. <a href="https://arxiv.org/abs/2205.11487" rel="external nofollow noopener" target="_blank">Link</a>.</li> </ul> <h2 id="other-comments">Other Comments</h2> <p>I hope you found this guide informative and useful. If you have any questions, notice any inaccuracies, or simply wish to discuss these fascinating models further, feel free to reach out in the comments or contact me directly. Your feedback and engagement are always appreciated!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/kl-mle/">The Dual Nature of MLE and KL Divergence in Generative Modeling</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Kelvin Christian. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-submenus",title:"submenus",description:"",section:"Navigation",handler:()=>{window.location.href="/_pages/dropdown/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-exploring-the-landscape-of-generative-models-a-simple-guide",title:"Exploring the Landscape of Generative Models: A Simple Guide",description:"Simple comparison between Autoregressive, Flow, VAEs, GANs, Energy-Based, Score-Based, and Diffusion models.",section:"Posts",handler:()=>{window.location.href="/blog/2023/generative-model/"}},{id:"post-the-dual-nature-of-mle-and-kl-divergence-in-generative-modeling",title:"The Dual Nature of MLE and KL Divergence in Generative Modeling",description:"Explaining the connection between MLE and KL Divergence in Generative Modeling",section:"Posts",handler:()=>{window.location.href="/blog/2023/kl-mle/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-10",title:"project 10",description:"A project with an introduction section",section:"Projects",handler:()=>{window.location.href="/projects/10_project/"}},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"projects-askcet",title:"ASKCET",description:"A product recognition app.",section:"Projects",handler:()=>{window.location.href="/projects/askcet/"}},{id:"projects-demat",title:"DeMAT",description:"Diffusion-enhanced Mask Aware Transformer for Deep Generative In-painting",section:"Projects",handler:()=>{window.location.href="/projects/demat/"}},{id:"projects-fine-tuning-stable-diffusion",title:"Fine-Tuning Stable Diffusion",description:"Application of LoRA and Dreambooth for fine-tuning Stable Diffusion",section:"Projects",handler:()=>{window.location.href="/projects/ft-stable-diffusion/"}},{id:"projects-isearch-hotels",title:"iSEArch Hotels",description:"Chatbot for hotel recommendations",section:"Projects",handler:()=>{window.location.href="/projects/hotel-chatbot/"}},{id:"projects-multi-subtask-policy-learning-in-robotic",title:"Multi-subtask Policy Learning in robotic",description:"Data-Efficient Multi-subtask Policy Learning for Robotic Manipulation",section:"Projects",handler:()=>{window.location.href="/projects/policy-learning/"}},{id:"projects-soba-up",title:"Soba-Up",description:"Be safe on a night out!",section:"Projects",handler:()=>{window.location.href="/projects/soba-up/"}},{id:"projects-tennis-shot-classification",title:"Tennis Shot Classification",description:"Trained TimeSformer model using a tennis video clip dataset",section:"Projects",handler:()=>{window.location.href="/projects/tennis-shot/"}},{id:"projects-text-to-table",title:"Text-to-Table",description:"Text-to-Table: Comparison between GPT and BART",section:"Projects",handler:()=>{window.location.href="/projects/text-to-table/"}},{id:"projects-trip-optimizer",title:"Trip Optimizer",description:"An automated trip planner given the restaurants and recreational spots data.",section:"Projects",handler:()=>{window.location.href="/projects/trip/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6B%65%6C%76%69%6E%63%68%72%69%73%74%69%61%6E%32%37%37@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>