<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Dual Nature of MLE and KL Divergence in Generative Modeling | Kelvin Christian </title> <meta name="author" content="Kelvin Christian"> <meta name="description" content="Explaining the connection between MLE and KL Divergence in Generative Modeling"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kelvinc123.github.io/blog/2023/kl-mle/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Kelvin Christian </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Dual Nature of MLE and KL Divergence in Generative Modeling</h1> <p class="post-meta"> November 05, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/mle"> <i class="fa-solid fa-hashtag fa-sm"></i> MLE</a>   <a href="/blog/tag/kl"> <i class="fa-solid fa-hashtag fa-sm"></i> KL</a>   <a href="/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> optimization</a>     ·   <a href="/blog/category/generative-models"> <i class="fa-solid fa-tag fa-sm"></i> generative-models</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>\(\DeclareMathOperator*{\argmin}{argmin}\) \(\DeclareMathOperator*{\argmax}{argmax}\)</p> <h2 id="kl-divergence">KL-Divergence</h2> <p>In generative modeling, we assume the existence of a true probability distribution \(p_{\text{data}}\) of our dataset. However, this distribution is usually unknown. We aim to approximate the true data distribution \(p_{\text{data}}\), with a model distribution \(p_{\theta}\), parameterized by \(\theta\), striving for the closest possible match. One solution to achieve this is to find \(\theta^{*}_{\text{KL}}\) that minimizes the <strong>Kullback–Leibler divergence</strong> between \(p_{\text{data}}\) and \(p_{\theta}\).</p> <p>Recall that the KL-divergence of the 2 probabilities \(p\) and \(q\) is given by:</p> \[D_{\text{KL}}(p || q) = \mathbb{E}_{\mathbf{x} \sim p(\mathbf{x})}\left[\log \left(\frac{p(\mathbf{x})}{q(\mathbf{x})}\right)\right]\] <p>with three properties:</p> <ol> <li> $$D_{\text{KL}}(p || q) \ge 0$$</li> <li> $$D_{\text{KL}}(p || q) = 0 \text{ if and only if } p=q$$</li> <li> $$D_{\text{KL}}(p || q) \ne D_{\text{KL}}(q || p)$$</li> </ol> <p><br> Our objective is to find the parameter \(\theta^{*}_{\text{KL}}\) that minimizes the function \(D_{\text{KL}}(p_{\text{data}} || p_{\theta})\). Note that \(D_{\text{KL}}(p_{\text{data}} || p_{\theta})\) is preferred over \(D_{\text{KL}}(p_{\theta} || p_{\text{data}})\) because the expectation in the former is with respect to \(p_{\text{data}}\), the true data distribution, whereas in the latter, it is with respect to \(p_{\theta}\). We can further expand the objective equation to:</p> \[\begin{align} \theta^{*}_{\text{KL}} &amp;= \argmin\limits_{\theta} D_{\text{KL}}(p_{\text{data}} || p_{\theta}) \\ &amp;= \argmin\limits_{\theta} \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}\left[\log \left(\frac{p_{\text{data}}(\mathbf{x})}{p_{\theta}(\mathbf{x})}\right)\right] \\ &amp;= \argmin\limits_{\theta} \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}\left[\log (p_{\text{data}}(\mathbf{x})) - \log (p_{\theta}(\mathbf{x}))\right] \\ &amp;= \argmin\limits_{\theta} \left(\mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}\left[\log (p_{\text{data}}(\mathbf{x}))] - \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log (p_{\theta}(\mathbf{x}))\right] \right) \\ &amp;= \argmin\limits_{\theta} - \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log (p_{\theta}(\mathbf{x}))] \\ &amp;= \argmax\limits_{\theta} \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log (p_{\theta}(\mathbf{x}))] \end{align}\] <p>The term \(\mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log (p_{\text{data}}(\mathbf{x}))]\) is a constant with respect to \(\theta\). Therefore, the equation \((3)\) is equivalent to the equation \((4)\). Since the last equation is in expectation, we can use sampling to calculate the empirical approximation of the expectation. Given the presupposition of a true underlying distribution \(p_{\text{data}}\), we can represent our dataset as a series of samples \(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(n)}\) drawn from \(p_{\text{data}}(\mathbf{x})\), which leads us to the following derivation:</p> \[\begin{align} \argmin\limits_{\theta} D_{\text{KL}}(p_{\text{data}} || p_{\theta}) = \argmax\limits_{\theta} \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log (p_{\theta}(\mathbf{x}))] \approx \argmax\limits_{\theta} \frac{1}{n} \sum_{i=1}^{n}\log (p_{\theta}(\mathbf{x}^{(i)})) \end{align}\] <h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2> <p>In parallel with our exploration of KL-divergence, Maximum Likelihood Estimation (MLE) offers a complementary perspective. MLE find the parameter \(\theta^{*}\) that maximizes the likelihood of observing the dataset under the model. More formally, given the i.i.d samples \(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(n)}\), we can write the MLE objective as:</p> \[\begin{align} \theta^{*}_{\text{MLE}} &amp;= \argmax\limits_{\theta} p_{\theta}(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(n)}) \\ &amp;= \argmax\limits_{\theta} \prod_{i=1}^{n}p_{\theta}(\mathbf{x}^{(i)}) \\ &amp;= \argmax\limits_{\theta} \log \left(\prod_{i=1}^{n}p_{\theta}(\mathbf{x}^{(i)}) \right) \\ &amp;= \argmax\limits_{\theta} \sum_{i=1}^{n} \log p_{\theta}(\mathbf{x}^{(i)}) \\ \end{align}\] <p>We can start to see the similarity in the formula.</p> <h2 id="relationship-between-mle-and-kl-divergence">Relationship between MLE and KL-Divergence</h2> <p>The equations above show the connection between MLE and KL-divergence. In MLE, our goal is to find the parameter set \(\theta^{*}_{\text{MLE}}\) that maximizes the log-likelihood of our observed dataset. Conversely, in the context of KL-divergence, we minimize the distance between the model distribution with the true data distribution. The connection can be shown as follows:</p> \[\begin{align} \theta^{*}_{\text{KL}} &amp;= \argmin\limits_{\theta} D_{\text{KL}}(p_{\text{data}} || p_{\theta}) \\ &amp;\approx \argmax\limits_{\theta} \frac{1}{n} \sum_{i=1}^{n}\log (p_{\theta}(\mathbf{x}^{(i)})) \\ &amp;= \argmax\limits_{\theta} \sum_{i=1}^{n}\log (p_{\theta}(\mathbf{x}^{(i)})) \\ &amp;= \theta^{*}_{\text{MLE}} \end{align}\] <p>The practical significance of this duality lies in its utility for constructing probabilistic models and understanding the core of machine learning inference. Doing the MLE of our model distribution ensures that our model’s assumptions reflect reality as closely as possible. Consider two probability models \(f\) and \(g\), parameterized by a set of parameters \(\theta\). The figure below represents the application of MLE to find the optimal parameters.</p> <p><img class="img-fluid rounded z-depth-1" src="/assets/img/kl-mle-figure.jpg" alt="KL-MLE Figure" data-zoomable=""></p> <p><br> The optimal parameters \(\theta^{*}\) for our models \(f\) and \(g\), namely \(f_{\theta^{*}}\) and \(g_{\theta^{*}}\), are those that minimizes the KL-divergence to \(p_{\text{data}}\). Therefore, when we employ MLE, we are implicitly minimizing the KL Divergence between our model’s distribution and the true distribution of the data.</p> <h2 id="limitations">Limitations</h2> <p>This approach, however, encounters a fundamental limitation due to the elusive nature of \(p_{\text{data}}\), making it impossible to directly compute \(D_{\text{KL}}(p_{\text{data}}||p_{\theta})\). While we can estimate \(\mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log (p_{\theta}(\mathbf{x}))]\) using sampling methods, the following expression:</p> \[D_{\text{KL}}(p_{\text{data}}||p_{\theta}) = \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}\left[\log \left(\frac{p_{\text{data}}(\mathbf{x})}{p_{\theta}(\mathbf{x})}\right)\right] = \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}\left[\log (p_{\text{data}}(\mathbf{x}))] - \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log (p_{\theta}(\mathbf{x}))\right]\] <p>Depends on the term \(\mathbb{E}_{\mathbf{x} \sim p_{\text{data}}(\mathbf{x})}[\log (p_{\text{data}}(\mathbf{x}))]\), which remains unknown. Future explorations must focus on developing innovative approaches or refining existing methods to better approximate or understand \(p_{\text{data}}\). Such advancements are crucial for enhancing the accuracy and reliability of model predictions, ultimately pushing the boundaries of our understanding in this field.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/generative-model/">Exploring the Landscape of Generative Models: A Simple Guide</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Kelvin Christian. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>