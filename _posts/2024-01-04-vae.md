---
layout: post
title: "Variational Autoencoders: From Bayes' Theorem to Generative Modeling"
description: A rigorous mathematical journey through VAEs, starting from conditional probability and building up to practical implementations.
tags: VAE generative-models deep-learning probability
giscus_comments: true
date: 2024-01-04
featured: true

authors:
  - name: Kelvin Christian

bibliography: 2018-12-22-distill.bib

toc:
  - name: Introduction
  - name: Foundations in Probability Theory
  - name: The Problem of Generative Modeling
  - name: Variational Inference
  - name: The VAE Framework
  - name: The Reparameterization Trick
  - name: Implementation in PyTorch
  - name: Applications and Extensions
  - name: Conclusion

# _styles: >
#   .algorithm {
#     background-color: #f8f9fa;
#     border: 1px solid #dee2e6;
#     border-radius: 0.25rem;
#     padding: 1rem;
#     margin: 1rem 0;
#   }
---

## Introduction

Variational Autoencoders (VAEs) <d-cite key="kingma2013auto"></d-cite> represent one of the most elegant intersections of probabilistic modeling and deep learning. Unlike traditional autoencoders that learn deterministic mappings, VAEs provide a principled framework for learning probabilistic latent variable models. This post will build VAEs from first principles, starting with conditional probability and carefully developing each component without mathematical gaps.

## Foundations in Probability Theory

### Conditional Probability and Bayes' Theorem

Let's start with the fundamental building blocks. Given two random variables $$X$$ and $$Z$, the conditional probability of $$X$$ given $$Z$$ is:

$$P(X|Z) = \frac{P(X, Z)}{P(Z)}$$

From this, we can derive Bayes' theorem:

$$P(Z|X) = \frac{P(X|Z)P(Z)}{P(X)}$$

where:
- $$P(Z|X)$$ is the **posterior** distribution
- $$P(X|Z)$$ is the **likelihood**
- $$P(Z)$$ is the **prior** distribution
- $$P(X)$$ is the **marginal likelihood** or **evidence**

### The Marginal Likelihood

The marginal likelihood is computed by marginalizing over all possible values of $$Z$:

$$P(X) = \int P(X, Z) dZ = \int P(X|Z)P(Z) dZ$$

This integral is often intractable for complex models, which is the core challenge VAEs address.

## The Problem of Generative Modeling

### Setting Up the Problem

Consider a dataset $$\mathcal{D} = \{x^{(1)}, x^{(2)}, ..., x^{(N)}\}$$ of observations (e.g., images). We assume each observation $$x^{(i)}$$ is generated from some unobserved latent variable $$z^{(i)}$$ through the following process:

1. Sample $$z \sim p(z)$$ from a prior distribution
2. Sample $$x \sim p_\theta(x|z)$$ from a conditional distribution

Our goal is to:
1. Learn the parameters $$\theta$$ of the generative model $$p_\theta(x|z)$
2. Infer the posterior distribution $$p_\theta(z|x)$$ for any given $$x$

### Maximum Likelihood Estimation

The natural approach is to maximize the log-likelihood of the data:

$$\log p_\theta(\mathcal{D}) = \sum_{i=1}^N \log p_\theta(x^{(i)})$$

For a single data point:

$$\log p_\theta(x) = \log \int p_\theta(x|z)p(z) dz$$

### The Intractability Problem

Computing $$p_\theta(x)$$ requires integrating over all possible values of $$z$. For high-dimensional $$z$$ and complex $$p_\theta(x|z)$$ (e.g., neural networks), this integral is intractable. Similarly, the posterior:

$$p_\theta(z|x) = \frac{p_\theta(x|z)p(z)}{p_\theta(x)}$$

is intractable due to the denominator.

## Variational Inference

### The Variational Lower Bound

To address intractability, we introduce an approximate posterior $$q_\phi(z|x)$$ with parameters $$\phi$. We'll derive a lower bound on $$\log p_\theta(x)$$ using Jensen's inequality.

Starting from the log-likelihood:

$$\begin{align}
\log p_\theta(x) &= \log \int p_\theta(x, z) dz \\
&= \log \int \frac{p_\theta(x, z)}{q_\phi(z|x)} q_\phi(z|x) dz \\
&= \log \mathbb{E}_{q_\phi(z|x)}\left[\frac{p_\theta(x, z)}{q_\phi(z|x)}\right]
\end{align}$$

Applying Jensen's inequality (since $$\log$$ is concave):

$$\log p_\theta(x) \geq \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)}\right]$$

### The Evidence Lower Bound (ELBO)

Let's expand this bound:

$$\begin{align}
\mathcal{L}(\theta, \phi; x) &= \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)}\right] \\
&= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x, z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)] \\
&= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] + \mathbb{E}_{q_\phi(z|x)}[\log p(z)] - \mathbb{E}_{q_\phi(z|x)}[\log q_\phi(z|x)] \\
&= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) \| p(z))
\end{align}$$

where $$D_{KL}$$ is the Kullback-Leibler divergence:

$$D_{KL}(q \| p) = \mathbb{E}_q\left[\log \frac{q}{p}\right]$$

### Tightness of the Bound

The gap between $$\log p_\theta(x)$$ and the ELBO is:

$$\log p_\theta(x) - \mathcal{L}(\theta, \phi; x) = D_{KL}(q_\phi(z|x) \| p_\theta(z|x))$$

**Proof:**

$$\begin{align}
\log p_\theta(x) &= \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x)] \\
&= \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{p_\theta(z|x)}\right] \\
&= \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)} \cdot \frac{q_\phi(z|x)}{p_\theta(z|x)}\right] \\
&= \mathcal{L}(\theta, \phi; x) + D_{KL}(q_\phi(z|x) \| p_\theta(z|x))
\end{align}$$

Since $$D_{KL} \geq 0$, maximizing the ELBO provides a lower bound on the log-likelihood.

## The VAE Framework

### Architecture Components

A VAE consists of two neural networks:

1. **Encoder** (recognition network): $$q_\phi(z|x)$
   - Maps input $$x$$ to parameters of the approximate posterior
   - Typically outputs mean $$\mu_\phi(x)$$ and variance $$\sigma^2_\phi(x)$$ for Gaussian $$q_\phi$

2. **Decoder** (generative network): $$p_\theta(x|z)$
   - Maps latent code $$z$$ to parameters of the data distribution
   - For continuous data: outputs mean (and optionally variance)
   - For binary data: outputs Bernoulli parameters

### Choice of Distributions

**Prior:** Typically $$p(z) = \mathcal{N}(0, I)$

**Approximate Posterior:** $$q_\phi(z|x) = \mathcal{N}(\mu_\phi(x), \text{diag}(\sigma^2_\phi(x)))$

For this choice, the KL divergence has a closed form:

$$D_{KL}(q_\phi(z|x) \| p(z)) = \frac{1}{2} \sum_{j=1}^J \left(1 + \log \sigma_j^2 - \mu_j^2 - \sigma_j^2\right)$$

where $$J$$ is the dimension of $$z$.

**Proof of KL divergence formula:**

For two multivariate Gaussians $$q = \mathcal{N}(\mu_1, \Sigma_1)$$ and $$p = \mathcal{N}(\mu_2, \Sigma_2)$:

$$D_{KL}(q \| p) = \frac{1}{2}\left[\log\frac{|\Sigma_2|}{|\Sigma_1|} - d + \text{tr}(\Sigma_2^{-1}\Sigma_1) + (\mu_2 - \mu_1)^T\Sigma_2^{-1}(\mu_2 - \mu_1)\right]$$

For our case with $$p(z) = \mathcal{N}(0, I)$$ and diagonal $$q_\phi$, this simplifies to the formula above.

## The Reparameterization Trick

### The Gradient Problem

To optimize the ELBO with respect to $$\phi$, we need:

$$\nabla_\phi \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$$

We cannot simply move the gradient inside the expectation because the distribution depends on $$\phi$.

### The Solution

Instead of sampling $$z \sim q_\phi(z|x)$, we reparameterize:

$$z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$

where $$\odot$$ denotes element-wise multiplication. Now:

$$\mathbb{E}_{q_\phi(z|x)}[f(z)] = \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I)}[f(\mu_\phi(x) + \sigma_\phi(x) \odot \epsilon)]$$

The gradient can now be moved inside:

$$\nabla_\phi \mathbb{E}_{\epsilon}[f(\mu_\phi(x) + \sigma_\phi(x) \odot \epsilon)] = \mathbb{E}_{\epsilon}[\nabla_\phi f(\mu_\phi(x) + \sigma_\phi(x) \odot \epsilon)]$$

## Implementation in PyTorch

Here's a complete implementation of a VAE for MNIST:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super(VAE, self).__init__()
        
        # Encoder
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc21 = nn.Linear(hidden_dim, latent_dim)  # Mean
        self.fc22 = nn.Linear(hidden_dim, latent_dim)  # Log variance
        
        # Decoder
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)
        
    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        return self.fc21(h1), self.fc22(h1)
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h3 = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))
    
    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def loss_function(recon_x, x, mu, logvar):
    """
    Computes the VAE loss function.
    KL(q(z|x) || p(z)) = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    """
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

def train(model, device, train_loader, optimizer, epoch):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = loss_function(recon_batch, data, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
        
        if batch_idx % 100 == 0:
            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '
                  f'({100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {loss.item() / len(data):.6f}')
    
    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')

def generate_samples(model, device, num_samples=10):
    """Generate new samples from the learned distribution"""
    model.eval()
    with torch.no_grad():
        # Sample from prior
        z = torch.randn(num_samples, 20).to(device)
        samples = model.decode(z).cpu()
        
        fig, axes = plt.subplots(1, num_samples, figsize=(15, 2))
        for i in range(num_samples):
            axes[i].imshow(samples[i].view(28, 28), cmap='gray')
            axes[i].axis('off')
        plt.show()

def visualize_latent_space(model, device, test_loader, num_batches=10):
    """Visualize the learned latent space"""
    model.eval()
    latents = []
    labels = []
    
    with torch.no_grad():
        for batch_idx, (data, label) in enumerate(test_loader):
            if batch_idx >= num_batches:
                break
            data = data.to(device)
            mu, _ = model.encode(data.view(-1, 784))
            latents.append(mu.cpu().numpy())
            labels.append(label.numpy())
    
    latents = np.concatenate(latents, axis=0)
    labels = np.concatenate(labels, axis=0)
    
    # Plot first two dimensions
    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(latents[:, 0], latents[:, 1], c=labels, cmap='tab10', alpha=0.5)
    plt.colorbar(scatter)
    plt.xlabel('Latent Dimension 1')
    plt.ylabel('Latent Dimension 2')
    plt.title('Latent Space Visualization')
    plt.show()

# Main training script
if __name__ == '__main__':
    # Hyperparameters
    batch_size = 128
    epochs = 10
    latent_dim = 20
    
    # Setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Data loading
    transform = transforms.Compose([transforms.ToTensor()])
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST('./data', train=False, transform=transform)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # Model
    model = VAE(latent_dim=latent_dim).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    # Training
    for epoch in range(1, epochs + 1):
        train(model, device, train_loader, optimizer, epoch)
        
        # Generate samples after each epoch
        if epoch % 2 == 0:
            print("Generating samples...")
            generate_samples(model, device)
    
    # Visualize latent space
    print("Visualizing latent space...")
    visualize_latent_space(model, device, test_loader)
```

### Understanding the Implementation

1. **Encoder Network**: Maps input to distribution parameters $$(\mu, \log \sigma^2)$
2. **Reparameterization**: Enables backpropagation through stochastic sampling
3. **Decoder Network**: Maps latent codes back to data space
4. **Loss Function**: Combines reconstruction loss and KL divergence

The reconstruction term ensures the model can reconstruct inputs, while the KL term regularizes the latent space to match the prior.

## Applications and Extensions

### 1. Image Generation

VAEs learn smooth latent representations enabling:
- **Interpolation**: Smooth transitions between images
- **Attribute manipulation**: Modifying specific features

```python
def interpolate(model, device, x1, x2, num_steps=10):
    """Interpolate between two images in latent space"""
    model.eval()
    with torch.no_grad():
        # Encode both images
        mu1, _ = model.encode(x1.view(-1, 784))
        mu2, _ = model.encode(x2.view(-1, 784))
        
        # Interpolate in latent space
        interpolations = []
        for alpha in np.linspace(0, 1, num_steps):
            z = (1 - alpha) * mu1 + alpha * mu2
            recon = model.decode(z)
            interpolations.append(recon.cpu().numpy())
        
        # Visualize
        fig, axes = plt.subplots(1, num_steps, figsize=(15, 2))
        for i, recon in enumerate(interpolations):
            axes[i].imshow(recon.reshape(28, 28), cmap='gray')
            axes[i].axis('off')
        plt.show()
```

### 2. Semi-Supervised Learning

By incorporating label information, VAEs can perform semi-supervised learning:

$$\mathcal{L} = \mathbb{E}_{q_\phi(z|x,y)}[\log p_\theta(x|z,y)] - D_{KL}(q_\phi(z|x,y) \| p(z|y))$$

### 3. Disentangled Representations

$\beta$-VAE <d-cite key="higgins2017beta"></d-cite> encourages disentangled representations:

$$\mathcal{L}_{\beta} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - \beta \cdot D_{KL}(q_\phi(z|x) \| p(z))$$

### 4. Conditional VAE (CVAE)

For conditional generation $$p(x|c)$:

```python
class ConditionalVAE(nn.Module):
    def __init__(self, input_dim, condition_dim, hidden_dim=400, latent_dim=20):
        super(ConditionalVAE, self).__init__()
        
        # Encoder
        self.fc1 = nn.Linear(input_dim + condition_dim, hidden_dim)
        self.fc21 = nn.Linear(hidden_dim, latent_dim)
        self.fc22 = nn.Linear(hidden_dim, latent_dim)
        
        # Decoder
        self.fc3 = nn.Linear(latent_dim + condition_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)
        
    def encode(self, x, c):
        inputs = torch.cat([x, c], dim=1)
        h1 = F.relu(self.fc1(inputs))
        return self.fc21(h1), self.fc22(h1)
    
    def decode(self, z, c):
        inputs = torch.cat([z, c], dim=1)
        h3 = F.relu(self.fc3(inputs))
        return torch.sigmoid(self.fc4(h3))
```

### 5. Applications in Other Domains

- **Drug Discovery**: Generating molecular structures
- **Text Generation**: Learning sentence embeddings
- **Music Generation**: Creating musical sequences
- **Anomaly Detection**: Using reconstruction error

## Advanced Topics

### Importance Weighted Autoencoders (IWAE)

IWAE <d-cite key="burda2015importance"></d-cite> uses multiple samples to tighten the bound:

$$\mathcal{L}_k = \mathbb{E}_{z_1,...,z_k \sim q_\phi(z|x)}\left[\log \frac{1}{k} \sum_{i=1}^k \frac{p_\theta(x, z_i)}{q_\phi(z_i|x)}\right]$$

### Normalizing Flows

Enhance the expressiveness of $$q_\phi(z|x)$$ using invertible transformations:

$$z_K = f_K \circ ... \circ f_1(z_0), \quad z_0 \sim q_0(z_0|x)$$

### Hierarchical VAEs

Use multiple layers of latent variables:

$$p(x, z_1, z_2) = p(x|z_1)p(z_1|z_2)p(z_2)$$

## Practical Considerations

### 1. Posterior Collapse

Sometimes the model ignores the latent code ($q_\phi(z|x) \approx p(z)$). Solutions:
- KL annealing: Gradually increase KL weight during training
- Free bits: Ensure minimum information in latent code
- Architectural changes: Skip connections, powerful decoders

### 2. Reconstruction Quality

VAEs often produce blurry reconstructions due to:
- Gaussian likelihood assumption
- Averaging effect of sampling
- Trade-off with KL regularization

### 3. Hyperparameter Tuning

Key hyperparameters:
- Latent dimension: Balance expressiveness and regularization
- Network architecture: Deeper networks for complex data
- Learning rate: Often requires careful scheduling
- $$\beta$$ weight: For $$\beta$-VAE variants

## Theoretical Connections

### Information Theory Perspective

The ELBO can be rewritten as:

$$\mathcal{L} = \mathbb{E}_{p_{\text{data}}(x)}[\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]] - I_q(X; Z)$$

where $$I_q(X; Z)$$ is the mutual information under $$q_\phi$.

### Connection to Rate-Distortion Theory

VAEs optimize a trade-off similar to rate-distortion:
- **Rate**: $$D_{KL}(q_\phi(z|x) \| p(z))$$ (bits to encode $$z$)
- **Distortion**: $$-\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]$$ (reconstruction error)

## Conclusion

Variational Autoencoders provide a principled framework for learning generative models by combining:
1. **Variational inference** for approximate posterior computation
2. **Neural networks** for flexible function approximation
3. **Reparameterization trick** for efficient gradient estimation

The mathematical foundation we've developed—from conditional probability through the ELBO derivation—reveals VAEs as a natural solution to the challenging problem of learning latent variable models. Their flexibility and theoretical grounding have made them a cornerstone of modern generative modeling, with applications ranging from image synthesis to drug discovery.

The beauty of VAEs lies not just in their practical success, but in how they unite probabilistic modeling with deep learning, providing both theoretical guarantees and empirical performance. As we continue to develop more sophisticated variants and applications, the core insights remain: by approximating intractable posteriors and optimizing tractable bounds, we can learn rich generative models of complex data.

## References

<d-bibliography></d-bibliography>

## Appendix: Mathematical Proofs

### Proof of ELBO as Lower Bound

**Theorem:** For any distributions $$p_\theta(x, z)$$ and $$q_\phi(z|x)$:
$$\log p_\theta(x) \geq \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)}\right]$$

**Proof:**
$$\begin{align}
\log p_\theta(x) &= \log \int p_\theta(x, z) dz \\
&= \log \int \frac{p_\theta(x, z)}{q_\phi(z|x)} q_\phi(z|x) dz \\
&\geq \int q_\phi(z|x) \log \frac{p_\theta(x, z)}{q_\phi(z|x)} dz \quad \text{(Jensen's inequality)} \\
&= \mathbb{E}_{q_\phi(z|x)}\left[\log \frac{p_\theta(x, z)}{q_\phi(z|x)}\right]
\end{align}$$

### Gaussian KL Divergence Derivation

For $$q = \mathcal{N}(\mu_1, \sigma_1^2 I)$$ and $$p = \mathcal{N}(\mu_2, \sigma_2^2 I)$:

$$\begin{align}
D_{KL}(q \| p) &= \int q(z) \log \frac{q(z)}{p(z)} dz \\
&= \mathbb{E}_q[\log q(z)] - \mathbb{E}_q[\log p(z)] \\
&= -\frac{d}{2}\log(2\pi) - \frac{1}{2}\sum_i \log \sigma_{1,i}^2 - \frac{d}{2} \\
&\quad + \frac{d}{2}\log(2\pi) + \frac{1}{2}\sum_i \log \sigma_{2,i}^2 + \frac{1}{2\sigma_{2}^2}\mathbb{E}_q[\|z - \mu_2\|^2] \\
&= \frac{1}{2}\sum_i \left[\log \frac{\sigma_{2,i}^2}{\sigma_{1,i}^2} - 1 + \frac{\sigma_{1,i}^2}{\sigma_{2,i}^2} + \frac{(\mu_{1,i} - \mu_{2,i})^2}{\sigma_{2,i}^2}\right]
\end{align}$$

For the special case where $$p = \mathcal{N}(0, I)$, this simplifies to our formula.
